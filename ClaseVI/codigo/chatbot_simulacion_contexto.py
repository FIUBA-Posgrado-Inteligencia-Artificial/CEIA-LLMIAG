"""
Chatbot Simple con Simulaci√≥n de Contexto usando Groq
===================================================

Este archivo implementa un chatbot b√°sico que demuestra conceptos fundamentales:
- Gesti√≥n de historial de conversaci√≥n
- Simulaci√≥n de contexto mediante acumulaci√≥n de mensajes
- Interfaz simple con Streamlit
- Integraci√≥n directa con la API de Groq

Diferencias con chatbot_gestionada.py:
- No usa LangChain (implementaci√≥n m√°s simple)
- Gesti√≥n manual del historial de conversaci√≥n
- Menos funcionalidades pero m√°s f√°cil de entender
- Ideal para entender los conceptos b√°sicos

Tecnolog√≠as utilizadas:
- Streamlit: Interfaz web simple
- Groq: API directa para acceso a LLMs
- Python: Listas y diccionarios para gesti√≥n de memoria

Autor: Clase VI - CEIA LLMIAG
Prop√≥sito: Demostrar conceptos b√°sicos de memoria conversacional

Instrucciones para ejecutar:
    streamlit run chatbot_simulacion_contexto.py

Requisitos:
    pip install streamlit groq

Variables de entorno necesarias:
    GROQ_API_KEY: Tu clave API de Groq
"""

# ========================================
# IMPORTACI√ìN DE LIBRER√çAS
# ========================================

import streamlit as st    # Framework para interfaz web
import os                # Para acceder a variables de entorno
from groq import Groq    # Cliente directo de Groq (sin LangChain)

# ========================================
# CONFIGURACI√ìN INICIAL Y AUTENTICACI√ìN
# ========================================

# Obtener la clave API de Groq desde las variables de entorno
# Esta es una pr√°ctica de seguridad para no hardcodear credenciales
groq_api_key = os.environ.get("GROQ_API_KEY")

# Validar que la clave API est√© disponible
if not groq_api_key:
    st.error("‚ö†Ô∏è GROQ_API_KEY no est√° configurada en las variables de entorno")
    st.info("üí° Configura tu clave API: export GROQ_API_KEY='tu-clave-aqui'")
    st.stop()

# Crear el cliente de Groq para comunicaci√≥n directa con la API
# Nota: Aqu√≠ usamos el cliente nativo de Groq, no LangChain
try:
    client = Groq(api_key=groq_api_key)
    st.sidebar.success("‚úÖ Cliente Groq conectado exitosamente")
except Exception as e:
    st.sidebar.error(f"‚ùå Error al conectar con Groq: {str(e)}")
    st.stop()

# ========================================
# GESTI√ìN DE MEMORIA CONVERSACIONAL
# ========================================

# Inicializar el historial de conversaci√≥n en el estado de la sesi√≥n de Streamlit
# st.session_state permite mantener datos entre ejecuciones de la aplicaci√≥n
if "conversation_history" not in st.session_state:
    # Formato de lista de diccionarios compatible con la API de Groq
    # Cada mensaje tiene: {"role": "user"/"assistant", "content": "texto"}
    st.session_state.conversation_history = []
    st.sidebar.info("üí¨ Nueva conversaci√≥n iniciada")
else:
    # Mostrar informaci√≥n del historial actual
    num_mensajes = len(st.session_state.conversation_history)
    st.sidebar.info(f"üí¨ Conversaci√≥n activa: {num_mensajes} mensajes")

# Bot√≥n para limpiar el historial de conversaci√≥n
if st.sidebar.button("üóëÔ∏è Limpiar Conversaci√≥n"):
    st.session_state.conversation_history = []
    st.sidebar.success("‚úÖ Conversaci√≥n reiniciada")
    st.rerun()  # Recargar la aplicaci√≥n


def generate_response(input_text):
    """
    Genera una respuesta del chatbot manteniendo el contexto de la conversaci√≥n.
    
    Esta funci√≥n demuestra la simulaci√≥n b√°sica de contexto:
    1. Agrega el mensaje del usuario al historial
    2. Env√≠a TODO el historial al modelo (simulaci√≥n de contexto)
    3. Recibe la respuesta del modelo
    4. Agrega la respuesta al historial
    5. Retorna la respuesta para mostrar
    
    Args:
        input_text (str): Texto ingresado por el usuario
        
    Returns:
        str: Respuesta generada por el modelo LLM
        
    Importante:
        - El contexto se simula enviando TODO el historial en cada llamada
        - Esto es diferente a tener "memoria real" como en LangChain
        - El modelo ve toda la conversaci√≥n previa cada vez
    """
    
    try:
        # ========================================
        # PASO 1: AGREGAR MENSAJE DEL USUARIO
        # ========================================
        
        # Agregar el nuevo mensaje del usuario al historial
        # Formato: {"role": "user", "content": "mensaje del usuario"}
        st.session_state.conversation_history.append({
            "role": "user", 
            "content": input_text
        })
        
        # ========================================
        # PASO 2: PREPARAR CONTEXTO COMPLETO
        # ========================================
        
        # El historial completo se env√≠a al modelo para simular contexto
        # Esto incluye todos los mensajes previos de user y assistant
        messages_for_api = st.session_state.conversation_history.copy()
        
        # Informaci√≥n de debug para estudiantes
        st.sidebar.caption(f"üì§ Enviando {len(messages_for_api)} mensajes al modelo")
        
        # ========================================
        # PASO 3: LLAMADA A LA API DE GROQ
        # ========================================
        
        # Realizar la llamada a la API con el historial completo
        # El modelo LLaMA procesar√° toda la conversaci√≥n para generar contexto
        chat_completion = client.chat.completions.create(
            messages=messages_for_api,           # Todo el historial de conversaci√≥n
            model="llama3-8b-8192",             # Modelo Llama 3 con contexto de 8192 tokens
            temperature=0.7,                    # Controla la creatividad (0=determinista, 1=creativo)
            max_tokens=1000,                    # M√°ximo de tokens en la respuesta
            top_p=0.9,                         # Control de diversidad en la generaci√≥n
        )
        
        # Extraer la respuesta del modelo
        response = chat_completion.choices[0].message.content
        
        # ========================================
        # PASO 4: AGREGAR RESPUESTA AL HISTORIAL
        # ========================================
        
        # Agregar la respuesta del assistant al historial para futuras referencias
        # Formato: {"role": "assistant", "content": "respuesta del modelo"}
        st.session_state.conversation_history.append({
            "role": "assistant", 
            "content": response
        })
        
        # Informaci√≥n adicional para el sidebar
        st.sidebar.success(f"‚úÖ Respuesta generada ({len(response)} caracteres)")
        
        return response
        
    except Exception as e:
        # Manejo de errores con informaci√≥n educativa
        error_msg = f"Error al generar respuesta: {str(e)}"
        st.sidebar.error(f"‚ùå {error_msg}")
        
        # Remover el √∫ltimo mensaje del usuario si hubo error
        if st.session_state.conversation_history and \
           st.session_state.conversation_history[-1]["role"] == "user":
            st.session_state.conversation_history.pop()
        
        return f"Lo siento, ocurri√≥ un error: {error_msg}"

# ========================================
# CONFIGURACI√ìN DE LA INTERFAZ PRINCIPAL
# ========================================

# Configurar el t√≠tulo y descripci√≥n de la aplicaci√≥n
st.title("ü§ñ Chatbot Simple con Simulaci√≥n de Contexto")
st.markdown("""
**Ejemplo educativo de chatbot b√°sico** que demuestra:
- üß† Simulaci√≥n de contexto mediante historial completo
- üìù Gesti√≥n manual de memoria conversacional
- üîó Integraci√≥n directa con API de Groq (sin LangChain)
- üí° Conceptos fundamentales de chatbots
""")

# Informaci√≥n del modelo en la barra lateral
st.sidebar.markdown("### üß† Configuraci√≥n del Modelo")
st.sidebar.info("""
**Modelo**: Llama 3 (8B par√°metros)
**Contexto**: 8,192 tokens
**Temperature**: 0.7 (equilibrado)
**Max Tokens**: 1,000
""")

# ========================================
# INTERFAZ DE ENTRADA DEL USUARIO
# ========================================

# Campo de entrada para el usuario
st.markdown("### üí¨ Escribe tu mensaje:")
user_input = st.text_input(
    "Usuario:",
    placeholder="Ejemplo: Hola, ¬øc√≥mo est√°s? ¬øDe qu√© hablamos antes?",
    label_visibility="collapsed"
)

# Bot√≥n adicional para enviar
col1, col2 = st.columns([4, 1])
with col2:
    send_button = st.button("üì§ Enviar", type="primary")

# ========================================
# PROCESAMIENTO Y VISUALIZACI√ìN
# ========================================

# Procesar la entrada del usuario
if user_input and (user_input.strip() or send_button):
    # Mostrar indicador de carga
    with st.spinner('ü§î Generando respuesta...'):
        response = generate_response(user_input)
    
    # Mostrar la respuesta
    st.markdown("### ü§ñ Respuesta del Chatbot:")
    st.markdown(f"""
    <div style="background-color: #f0f8ff; padding: 15px; border-radius: 10px; border-left: 4px solid #1f77b4;">
        {response}
    </div>
    """, unsafe_allow_html=True)

# ========================================
# MOSTRAR HISTORIAL DE CONVERSACI√ìN
# ========================================

# Panel expandible con el historial completo
if st.session_state.conversation_history:
    with st.expander(f"üìú Ver Historial Completo ({len(st.session_state.conversation_history)} mensajes)"):
        for i, message in enumerate(st.session_state.conversation_history):
            role = "üë§ Usuario" if message["role"] == "user" else "ü§ñ Chatbot"
            st.markdown(f"**{role}**: {message['content']}")
            if i < len(st.session_state.conversation_history) - 1:
                st.markdown("---")

# ========================================
# INFORMACI√ìN EDUCATIVA PARA ESTUDIANTES
# ========================================

# Panel expandible con informaci√≥n t√©cnica
with st.expander("üìö Informaci√≥n Educativa - Simulaci√≥n de Contexto"):
    st.markdown("""
    ### ¬øC√≥mo funciona la simulaci√≥n de contexto?
    
    **1. Gesti√≥n Manual del Historial:**
    - Se mantiene una lista de todos los mensajes
    - Cada mensaje tiene formato: `{"role": "user/assistant", "content": "texto"}`
    - El historial se almacena en `st.session_state`
    
    **2. Simulaci√≥n de Contexto:**
    - En cada consulta se env√≠a TODO el historial al modelo
    - El modelo procesa toda la conversaci√≥n previa
    - Esto simula "memoria" pero no es memoria real
    
    **3. Diferencias con LangChain:**
    - **Este enfoque**: API directa, implementaci√≥n simple
    - **LangChain**: Gesti√≥n autom√°tica, m√°s funcionalidades
    
    **4. Limitaciones de este Enfoque:**
    - Costo creciente (m√°s tokens en cada llamada)
    - L√≠mite de contexto del modelo (8,192 tokens)
    - No hay optimizaci√≥n de memoria
    
    **5. Ventajas Educativas:**
    - F√°cil de entender
    - Control total sobre el proceso
    - Transparencia en el funcionamiento
    
    ### Estructura de Datos:
    ```python
    conversation_history = [
        {"role": "user", "content": "Hola"},
        {"role": "assistant", "content": "¬°Hola! ¬øC√≥mo est√°s?"},
        {"role": "user", "content": "¬øDe qu√© hablamos?"},
        # ... m√°s mensajes
    ]
    ```
    
    ### Flujo de Procesamiento:
    ```
    Usuario ‚Üí Input ‚Üí Agregar a historial ‚Üí Enviar todo a API ‚Üí 
    Respuesta ‚Üí Agregar respuesta ‚Üí Mostrar resultado
    ```
    """)

# ========================================
# PIE DE P√ÅGINA EDUCATIVO
# ========================================

st.markdown("---")
st.markdown("""
**üìñ Clase VI - CEIA LLMIAG** | Ejemplo de simulaci√≥n b√°sica de contexto conversacional

üí° **Pr√≥ximo paso**: Compara este enfoque con `chatbot_gestionada.py` para ver las diferencias con LangChain
""")

# Informaci√≥n de debug para desarrollo
if st.sidebar.checkbox("üîß Modo Debug (para desarrolladores)"):
    st.sidebar.markdown("### Debug Info:")
    st.sidebar.json({
        "total_mensajes": len(st.session_state.conversation_history),
        "ultimo_mensaje": st.session_state.conversation_history[-1] if st.session_state.conversation_history else "Ninguno",
        "tokens_aproximados": sum(len(msg["content"]) for msg in st.session_state.conversation_history) // 4
    })
